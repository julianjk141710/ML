# 吴恩达DL

## 第一周 Linear Regression with One Variable

### Model and Cost Function 模型和代价函数

#### Model Representation 模型展示

m用来表示训练样本的数量

x 表示输入变量（或者叫做特征）

y 表示目标变量

(x, y)表示一个训练样本

(x^(i), y^(i))表示第i个训练样本 不表示幂

*h*表示假设函数（hypothesis），即用来预测的函数
$$
h_\theta(x) = \theta_0 + \theta_1x
$$
（有时候θ会被省略）

![监督学习过程](..\监督学习过程.JPG)

#### Cost Function 代价函数

$$
J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i = 1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})}^2
$$

代价函数也被称为平方误差函数 / 平方误差代价函数，我们要做的就是求出
$$
minJ(\theta_0, \theta_1)
$$
对于大多数Regression问题中，Square error function works well。

#### Gradient descent algorithm 梯度下降

$$
\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta_0, \theta_1)  （对于j = 0 和 j = 1）
$$

:=表示赋值

α 为学习率（始终是一个正数），控制我们以多大的幅度更新参数，越大更新的速度越快，越慢则更新的速度越慢。

梯度下降更新需要注意的一点是上面公式里的两个参数要进行同时更新，正确做法如下图所示。![梯度下降正确做法](..\梯度下降正确做法.JPG)

### Linear Algebra Review 线性代数回顾（真的被回顾到了... 丢人）

向量（Vector）是一个n × 1 的矩阵

通常用大写字母表示矩阵

本课程中对于向量的索引默认是1开头索引 

不是所有的矩阵都有逆

### Multivariate Linear Regression 多元线性回归

#### Multiple Features 多特征量

$$
n 表示特征量的数目(注意和之前m的区别)
$$

$$
x^{(i)}表示第i个特征向量
$$

$$
x^{(i)}_j表示第i个特征向量的第j项
$$

![多元变量函数的一些基本概念](..\多元变量函数的一些基本概念.JPG)

有多个特征量的假设函数写成：
$$
h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 + ... + \theta_nx_n
$$
为了方便表示，我们定义
$$
x_0 = 1
$$
即对于任何一个特征向量，它的第一项
$$
x^{(i)}_0 = 1
$$
为了简化表示，写成矩阵乘法:
$$
h_\theta(x) = \theta^TX
$$
起名：多元线性回归

#### 多元梯度下降法

由于特征值增加，梯度函数也有一些改变
$$
\theta_j := \theta_j - \sum^m_1(h_\theta(x^{(i)}) - y^{(i)})x^{(i)}_j
$$
其中由于我们之前定义了
$$
x^{(i)}_0 = 1
$$
所以对于第0项（θ0）这个公式也是正确的

#### 特征缩放

如果不同特征值的取值在相近的范围内的话，梯度下降就可以更快的收敛，为了达到这一目的，我们采用对特征进行缩放的方法。
$$
用x_i - \mu_i代替x_i使得特征值具有为0的平均值
$$

$$
x_i << \frac{x_i - \mu_i}{s_i}\quad
$$

$$
(\mu_i表示数据集的平均值，s_i表示数据的取值范围 即max-min)
$$

#### 学习率

如果学习率过小，梯度函数收敛速度就会很慢，但是如果梯度函数过大，就会出现并不是每次迭代都会减小甚至是不收敛的情况

取学习率的做法：
$$
\cdots\quad0.001, 0.003,0.01,0.03,0.1,0.3,1\cdots
$$
大约3倍取一个数字

#### 正规方程法求解假设函数的参数θ

可以避免迭代，不需要设置学习率，但是不能应付特征值很大（10000以上）的假设函数，因为时间复杂度太高（3次方）

![正规方程法求解](..\正规方程法求解.JPG)

![正规方程法求解2](..\正规方程法求解2.JPG)

### Octave语法

不等：~= 

e.g. 1 ~= 2

异或：xor(a, b)

e.g. xor(1, 0)

#### 建立矩阵

A = [1 2; 3 4; 5 6]

输出：

A =

   1   2
   3   4
   5   6

#### 建立向量

行向量：v = [1 2 3]

列向量：v = [1; 2; 3]

#### 通过步长建立向量

v = 1: 0.1 : 2 从1开始到2，步长为0.1

v =

    1.0000    1.1000    1.2000    1.3000    1.4000    1.5000    1.6000    1.7000    1.8000    1.9000    2.0000

#### 建立所有元素都是1的矩阵

ones(2, 3)

ans =

   1   1   1
   1   1   1

#### 画直方图

hist(变量， 数字)   数字表示画多少个柱子

#### 生成单位矩阵

eye(x) x表示矩阵的阶数

eye(4) 

ans =

Diagonal Matrix

   1   0   0   0
   0   1   0   0
   0   0   1   0
   0   0   0   1

#### 获取矩阵大小

size(A) A表示矩阵 会返回一个1×2的矩阵 分别表示行和列

#### 获取向量长度

length(v) v表示向量

#### 载入数据

load 文件名 或者写成 load (字符串)

在Octave中，字符串是由单引号承载的

e.g. 'apple'

#### 查看Octave内存中存好的变量

who

#### 查看文件内容

直接输入文件名即可

#### 删除变量

clear 变量名

#### 保存变量到磁盘

save xxx.mat 变量

e.g. save hello.mat v

保存成其他格式也是可以的

e.g. save hello.txt v -ascii

#### 索引矩阵中的元素（可以用这种方式赋值）

A(x, y) 取出第x行 第y列的元素

A(x,:) 取出第x行所有元素

A(:,y) 取出第y列所有元素

赋值

e.g. A(:,2) = [11; 12; 13] 将A矩阵第2列元素替换成11 12 13

#### 增加一列向量

A = [A, [7; 8; 9]] 在A矩阵的右侧增加一个列向量

#### 把矩阵中所有元素转化成一个列向量

A(:)

#### 左右拼接两个矩阵

C  = [A  B]

#### 上下拼接两个矩阵

C  = [A; B]

### Classification and Representation 分类和表示

#### Logistic Regression Model

Sigmoid函数 也称作S增长曲线（生物里见过）
$$
g(z) = \frac{1}{1+e^{-z}}
$$
Logistic Regression模型：
$$
h_\theta(x) = g(\theta^Tx)
$$
即
$$
h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx}}
$$

#### 决策界限

#### ![logistic回归模型详解](..\logistic回归模型详解.JPG)

根据图像可知，预测函数值大于等于0.5和计算出来的自变量值大于等于0是一回事，小于0.5和计算出来的自变量值小于0是一回事。

#### Logistic Function的代价函数

$$
J(\theta) = \frac{1}{m}\sum^m_{i = 1}Cost(h_\theta(x^{(i)}),y^i)
$$

$$
Cost(h_\theta(x),y) = \begin{cases}-log(h_\theta(x)) \quad if \quad y = 1\\-log(1-h_\theta(x))\quad if\quad y = 0\end{cases}
$$

y = 1时函数的图像：![logistic代价函数图像1](..\logistic代价函数图像1.JPG)

y = 0时函数的图像：![logistic代价函数图像2](..\logistic代价函数图像2.JPG)

可以简写单项代价函数为一个函数：
$$
Cost(h_\theta(x),y) = -ylog(h_\theta(x)) - (1-y)log(1-h_\theta(x))
$$
Logistic Function 的梯度下降更新参数
$$
\theta_j := \theta_j - \alpha\sum^m_{i = 1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j
$$

梯度下降向量化表示：
$$
\frac{\partial}{\partial\theta}J(\theta) = \frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j
$$


#### 多元分类

即去除其中一类数据，把其他数据看作一类，分别建立预测函数，当给出一个需要预测的数据时，对每个预测函数进行求值然后选择最大的那一个

### 正则化

#### 过拟合

过拟合曲线会经过所有训练数据点，但是对于新的数据却不一定拟合的好。

#### 新的代价函数

为了避免过拟合现象的情况发生，我们更新一下代价函数，在代价函数的后面加一个“正则项”，目的是尽量减少参数θ的大小，这样可以使得拟合函数更简洁并且过拟合的可能性更小。

线性回归添加了正则化后的代价函数：
$$
J(\theta_0, \theta_1) = \frac{1}{2m}[\sum_{i = 1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})}^2 + \lambda\sum^n_{j = 1}\theta_j^2]
$$
添加的一项叫做正则化项，λ是正则化参数，为了使所有参数（θ）变小，它的作用是平衡两个目标：

1. 假设函数对数据集的拟合
2. 假设函数中的参数尽量小

#### 线性回归的正则化

#### ![正则化线性回归梯度下降更新](..\正则化线性回归梯度下降更新.JPG)

#### Logistic函数的正则化

![正则化Logistic函数梯度下降更新](..\正则化Logistic函数梯度下降更新.JPG)

### 神经网络

#### 神经网络模型展示

![神经网络原理图解](..\神经网络原理图解.JPG)
$$
a^{(j)}_i表示第j层的第i个激活项
$$
$$
\theta^{(j)}是权重矩阵，控制着某一层到下一层的映射关系
$$

矩阵θ的维度取决于前后两层神经元的维度，前一层维度为s，后一层维度为s'，那么矩阵θ的维度为s' × （s + 1） （加上1是因为s还有一个x0没画在上图中）

![前向传播](..\前向传播.JPG)

### 神经网络代价函数

#### 有多个输出的神经网络代价函数

![神经网络代价函数](..\神经网络代价函数.JPG)

$$
(h_\theta(x))_i表示输出层的第i项
$$
注意后面的正则项是从1开始算的

#### 数据集中只有一组数据的反向传播算法

![反向传播算法](..\反向传播算法.JPG)
$$
对于g'(z^{(3)})来说，它等于a^{(3)}.*(1-a^{(3)})
$$

#### 数据集中有m组数据的反向传播算法![m组数据反向传播](..\m组数据反向传播.JPG)

$$
1.首先将\Delta^{(l)}_{ij}设置为0（对所有的l, i,j）\\
2.从i=1到m遍历\quad\quad\quad\quad\quad\quad\quad\\
a)首先设置a^{(1)}=x^{(1)}\quad\quad\quad\quad\\
b)用前向传播算法计算a^{(l)}，\quad\quad\\对于l=2，3，4，…L（共L层）\\
c)用y^{(i)}计算\delta^{(L)} = a^{(L)}-y^{(i)}\quad\\
d)依次计算\delta^{(L-1)},\delta^{(L-2)}...到\delta^{(2)}\\ (注意没有\delta^{(1)}因为我们不认为输入数据x^{(i)})有错误
$$



红色部分是向量写法

j = 0时对应的是偏置项，所以没有加正则项

最后将D的项算出来之后，就是损失函数对每个参数θ的偏导了

#### 随机初始化参数θ

相同的初始化参数会使得训练的特征变得一样，为了使训练的特征彼此不同，用随机的方式初始化参数。

e.g. Theta1 = rand(10, 11) * (2 * INIT_EPSILON) - INIT_EPSILON;

rand(10, 11)会返回一个10 * 11 的随机矩阵，每个元素的大小在0-1之间，EPSILON是预先设定好的一个值，最后使得初始化出来的矩阵每个元素的大小都在 ±EPSILON 之间。

### 评估机器学习

#### 模型选择和训练、验证、测试集

将数据集按照6：2：2的比例分成三份，一份作为测试集，一份作为验证集(Cross Validation)，最后一部分作为测试集。分别对不同次数的多项式函数进行训练，并用验证集验证哪个多项式拟合的最好。

#### 诊断偏差(BIAS)和方差(VARIANCE)

![方差和偏差](D:\ML\ML\方差和偏差.JPG)

横轴：多项式次数

纵轴：与实际标签的偏差

下面的曲线对应着训练集的误差曲线，上面的曲线对应的是交叉测试集的曲线。对于欠拟合的假设函数，它的训练集误差和交叉测试集误差都会比较大，对于过拟合的假设函数，训练集误差较小，交叉测试集误差将远大于训练集误差。

#### 如何选择正则化参数λ

1. 用Jθ算出每个多项式假设函数的θ
2. 用交叉测试集依次验证并选出最好的一组θ
3. 用测试集验证误差

注意此处的交叉测试集函数和测试机函数都无正则化项

#### 绘制学习曲线

![高偏差学习曲线](D:\ML\ML\高偏差学习曲线.JPG)

在高偏差（欠拟合）下交叉测试集的偏差曲线和训练集的偏差曲线，最终两者会很接近并且都有着很高的偏差。

并且从图中可以看出，当一个假设函数已经处于高偏差状态，增加数据集的数量并不会使得优化效果变好。

![高方差学习曲线](D:\ML\ML\高方差学习曲线.JPG)

高方差学习曲线中，两个曲线会随着数据集的增大而逐渐靠近因此增大数据集的数量也许队训练是有好处的。

![获得更好的假设函数的修正方法](D:\ML\ML\获得更好的假设函数的修正方法.JPG)

有时候预测的值中有一类占比非常小，这时候预测的准确率即使很高也不代表预测的好，例如1000人里5个人得了癌症，但是预测正确律是98%，那么就有20人被预测为癌症，这显然不是好的预测。

引入查准率和召回率的概念（通常将少的那一类的预测结果看成1 即y = 1）

![查准率和召回率](D:\ML\ML\查准率和召回率.JPG)

若一个预测结果拥有好的查准率和召回率，那么可以确定这个预测算法是好的。

查准率 = 预测准的1 / 预测的1 = 预测准的1 / （预测准的1 + 预测为1但实际上是0）

召回率 = 预测准的1 / 实际上的1 = 预测准的1 / （预测准的1 + 预测为0但实际上是1）

#### 查准率和召唤率之间的权衡

引入新概念：F值 或者叫做F1值
$$
F_1 \quad Score: 2\frac{PR}{P+ R}
$$
F值越高，该函数越好

### SVM 支持向量机

#### SVM的损失函数

![SVM代价函数](D:\ML\ML\SVM代价函数.JPG)

把log(sigmoid)换成了另一个函数 并且将正则化参数λ取消 换成了在第一个求和项之前的参数C。

![支持向量机的假设函数](D:\ML\ML\支持向量机的假设函数.JPG)

与Logistic Regression不同的是，SVM的假设函数不会输出一个概率，而是会直接输出预测结果（是1还是0），如果θT * X ≥0，那么预测结果为1，否则预测结果为0。

![SVM代价函数的两个cost函数](D:\ML\ML\SVM代价函数的两个cost函数.JPG)

两个cost函数，如果y = 1，那么为了使损失函数小，就希望θT * X ≥ 1（而不仅仅是≥0，因为≥ 1的时候cost函数为0，y = 0时同理）。

SVM又称为大间隔分类器

![核函数和相似函数](D:\ML\ML\核函数和相似函数.JPG)

引入核函数的概念，即选取几个点 l ，并计算x到这个点的欧氏距离，如果二者距离很近，带入函数之后会得到1，否则得到0。

![核函数举例](D:\ML\ML\核函数举例.JPG)

根据深蓝色笔迹所做出的假设，靠近 l1 和 l2 两个点的点带入后会使得整个代数式的值大于0，因此放在假设函数中就会得到1，离 l1 和 l2 两个点比较远的点带入后会使得整个代数式的值小于0，因此放在假设函数中就会得到0。根据这个原理就可以得到很好的非线性的预测函数。

![参数l的选取](D:\ML\ML\参数l的选取.JPG)

首先将所有的标记点 l 设置为样本集x，即
$$
l^{(i)} = x^{(i)}对于某个样本x来说，由于一共有m个样本，就有m个 l，因此就可以算出m个f (f是x 和 l之间的相似度)
$$
$$
f_1 = sim(x, l^{(1)})\\
f_2 = sim(x, l^{(2)})\\...\\
f_m = sim(x, l^{(m)})
$$
对于某个样本x，算出的所有f，可以写成一个列向量的形式。
$$
f = [f_1, f_2,...,f_m]^T
$$
对于所有的样本x，就有m个列向量f
$$
x^{(i)}  ---- f^{(i)}----[f^{(i)}_1, f^{(i)}_2,...,f^{(i)}_n]^T
$$
![关于核函数的代价函数](D:\ML\ML\关于核函数的代价函数.JPG)

计算出所有的f之后，将代价函数中原本的x换成现在计算出的f，由于 f 的维度是 m * 1 ，θ 的维度也应该是 m * 1 并且注意后面的正则项是从 1 加到 m 。

![核函数参数选择](D:\ML\ML\核函数参数选择.JPG)

## 无监督学习

### K-Means

### ![k-means算法初步](D:\ML\ML\k-means算法初步.JPG)

K-Means算法的输入是

1. K 表示簇/族的个数
2. 一个不带标签的训练集

注意这里的第i个数据x^(i)是不带有偏置项x0的

![K-means算法步骤](D:\ML\ML\K-means算法步骤.JPG)

1.首先随机生成K个聚类中心

2.重复下面两件事：

1. 对于所有的样本，计算每个样本到每个聚类中心的距离，并记下最近的那个聚类中心的索引，记成c^(i)，c^(i)的值应该在1...K之间（闭区间）即c^(i)表示当前样本所属的簇的索引
2. 对于每个聚类中心，计算属于这个聚类中心的点的坐标平均值



如果出现了有一个聚类点没有数据点属于它，一般会将这点删除，或者重新随机初始化这个点。（前者是比较常用的方式）

![K-means的失真代价函数](D:\ML\ML\K-means的失真代价函数.JPG)
$$
\mu_{c^{(i)}}表示的是第i个聚类点的坐标
$$
K-Means失真损失函数：
$$
J(c^{(1)},...,c^{(m)},\mu_1,...,\mu_K) = \frac{1}{m}\sum_{i=1}^m||x^{(i)}-\mu_{c^{(i)}}||^2
$$
如何选择初始聚类点

可以通过选择已有的数据点作为初始的聚类点，但这种情况可能会使得最后的结果是一个局部最优解而不是全局最优解。

可以通过多次随机初始化聚类点并在所有情况中选择一个代价函数最小的作为聚类结果，但是这种情况对于聚类点较少（<10）的情况比较好。 

### PCA

首先要对数据进行预处理

![PCA预处理](D:\ML\ML\PCA预处理.JPG)

![PCA算法](D:\ML\ML\PCA算法.JPG)

首先计算出协方差矩阵Σ（注意是矩阵），在matlab中可以写成

```matlab
Sigma = (1/m) * X' * X
```


$$
\Sigma = \frac{1}{m}\sum^n_{i = 1}(x^{(i)})(x^{(i)})^T
$$
 然后用matlab带有的svd函数求出U,S,V三个矩阵，我们需要的是矩阵U，U也是一个n×n的矩阵，如果要把n维向量降维成k维，那么取前k个列向量即可。这k个向量就是我们需要投影的k个方向。

截取完前k个列向量，就获得了一个n×k的矩阵。

然后用U（截取后的）的转置乘X就能得到投影后的矩阵Z。
$$
Z = U^TX
$$

### 如何选择PCA的K值

根据svd函数得到的S矩阵（是一个对角矩阵，即除了对角线上的元素外都是0），满足如下公式即可：
$$
1-\frac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}}\leq 0.01
$$
这个公式意味着保留了99%以上的方差。

### 将压缩后的k维向量还原成n维

$$
z = U_{reduce}^Tx\\
x_{approx}=U_{reduce}z
$$

