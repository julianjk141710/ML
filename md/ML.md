# 吴恩达DL

## 第一周 Linear Regression with One Variable

### Model and Cost Function 模型和代价函数

#### Model Representation 模型展示

m用来表示训练样本的数量

x 表示输入变量（或者叫做特征）

y 表示目标变量

(x, y)表示一个训练样本

(x^(i), y^(i))表示第i个训练样本 不表示幂

*h*表示假设函数（hypothesis），即用来预测的函数
$$
h_\theta(x) = \theta_0 + \theta_1x
$$
（有时候θ会被省略）

![监督学习过程](..\监督学习过程.JPG)

#### Cost Function 代价函数

$$
J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i = 1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})}^2
$$

代价函数也被称为平方误差函数 / 平方误差代价函数，我们要做的就是求出
$$
minJ(\theta_0, \theta_1)
$$
对于大多数Regression问题中，Square error function works well。

#### Gradient descent algorithm 梯度下降

$$
\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta_0, \theta_1)  （对于j = 0 和 j = 1）
$$

:=表示赋值

α 为学习率（始终是一个正数），控制我们以多大的幅度更新参数，越大更新的速度越快，越慢则更新的速度越慢。

梯度下降更新需要注意的一点是上面公式里的两个参数要进行同时更新，正确做法如下图所示。![梯度下降正确做法](..\梯度下降正确做法.JPG)

### Linear Algebra Review 线性代数回顾（真的被回顾到了... 丢人）

向量（Vector）是一个n × 1 的矩阵

通常用大写字母表示矩阵

本课程中对于向量的索引默认是1开头索引 

不是所有的矩阵都有逆

### Multivariate Linear Regression 多元线性回归

#### Multiple Features 多特征量

$$
n 表示特征量的数目(注意和之前m的区别)
$$

$$
x^{(i)}表示第i个特征向量
$$

$$
x^{(i)}_j表示第i个特征向量的第j项
$$

![多元变量函数的一些基本概念](..\多元变量函数的一些基本概念.JPG)

有多个特征量的假设函数写成：
$$
h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 + ... + \theta_nx_n
$$
为了方便表示，我们定义
$$
x_0 = 1
$$
即对于任何一个特征向量，它的第一项
$$
x^{(i)}_0 = 1
$$
为了简化表示，写成矩阵乘法:
$$
h_\theta(x) = \theta^TX
$$
起名：多元线性回归

#### 多元梯度下降法

由于特征值增加，梯度函数也有一些改变
$$
\theta_j := \theta_j - \sum^m_1(h_\theta(x^{(i)}) - y^{(i)})x^{(i)}_j
$$
其中由于我们之前定义了
$$
x^{(i)}_0 = 1
$$
所以对于第0项（θ0）这个公式也是正确的

#### 特征缩放

如果不同特征值的取值在相近的范围内的话，梯度下降就可以更快的收敛，为了达到这一目的，我们采用对特征进行缩放的方法。
$$
用x_i - \mu_i代替x_i使得特征值具有为0的平均值
$$

$$
x_i << \frac{x_i - \mu_i}{s_i}\quad
$$

$$
(\mu_i表示数据集的平均值，s_i表示数据的取值范围 即max-min)
$$

#### 学习率

如果学习率过小，梯度函数收敛速度就会很慢，但是如果梯度函数过大，就会出现并不是每次迭代都会减小甚至是不收敛的情况

取学习率的做法：
$$
\cdots\quad0.001, 0.003,0.01,0.03,0.1,0.3,1\cdots
$$
大约3倍取一个数字

#### 正规方程法求解假设函数的参数θ

可以避免迭代，不需要设置学习率，但是不能应付特征值很大（10000以上）的假设函数，因为时间复杂度太高（3次方）

![正规方程法求解](..\正规方程法求解.JPG)

![正规方程法求解2](..\正规方程法求解2.JPG)

### Octave语法

不等：~= 

e.g. 1 ~= 2

异或：xor(a, b)

e.g. xor(1, 0)

#### 建立矩阵

A = [1 2; 3 4; 5 6]

输出：

A =

   1   2
   3   4
   5   6

#### 建立向量

行向量：v = [1 2 3]

列向量：v = [1; 2; 3]

#### 通过步长建立向量

v = 1: 0.1 : 2 从1开始到2，步长为0.1

v =

    1.0000    1.1000    1.2000    1.3000    1.4000    1.5000    1.6000    1.7000    1.8000    1.9000    2.0000

#### 建立所有元素都是1的矩阵

ones(2, 3)

ans =

   1   1   1
   1   1   1

#### 画直方图

hist(变量， 数字)   数字表示画多少个柱子

#### 生成单位矩阵

eye(x) x表示矩阵的阶数

eye(4) 

ans =

Diagonal Matrix

   1   0   0   0
   0   1   0   0
   0   0   1   0
   0   0   0   1

#### 获取矩阵大小

size(A) A表示矩阵 会返回一个1×2的矩阵 分别表示行和列

#### 获取向量长度

length(v) v表示向量

#### 载入数据

load 文件名 或者写成 load (字符串)

在Octave中，字符串是由单引号承载的

e.g. 'apple'

#### 查看Octave内存中存好的变量

who

#### 查看文件内容

直接输入文件名即可

#### 删除变量

clear 变量名

#### 保存变量到磁盘

save xxx.mat 变量

e.g. save hello.mat v

保存成其他格式也是可以的

e.g. save hello.txt v -ascii

#### 索引矩阵中的元素（可以用这种方式赋值）

A(x, y) 取出第x行 第y列的元素

A(x,:) 取出第x行所有元素

A(:,y) 取出第y列所有元素

赋值

e.g. A(:,2) = [11; 12; 13] 将A矩阵第2列元素替换成11 12 13

#### 增加一列向量

A = [A, [7; 8; 9]] 在A矩阵的右侧增加一个列向量

#### 把矩阵中所有元素转化成一个列向量

A(:)

#### 左右拼接两个矩阵

C  = [A  B]

#### 上下拼接两个矩阵

C  = [A; B]

### Classification and Representation 分类和表示

#### Logistic Regression Model

Sigmoid函数 也称作S增长曲线（生物里见过）
$$
g(z) = \frac{1}{1+e^{-z}}
$$
Logistic Regression模型：
$$
h_\theta(x) = g(\theta^Tx)
$$
即
$$
h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx}}
$$

#### 决策界限

#### ![logistic回归模型详解](..\logistic回归模型详解.JPG)

根据图像可知，预测函数值大于等于0.5和计算出来的自变量值大于等于0是一回事，小于0.5和计算出来的自变量值小于0是一回事。

#### Logistic Function的代价函数

$$
J(\theta) = \frac{1}{m}\sum^m_{i = 1}Cost(h_\theta(x^{(i)}),y^i)
$$

$$
Cost(h_\theta(x),y) = \begin{cases}-log(h_\theta(x)) \quad if \quad y = 1\\-log(1-h_\theta(x))\quad if\quad y = 0\end{cases}
$$

y = 1时函数的图像：![logistic代价函数图像1](..\logistic代价函数图像1.JPG)

y = 0时函数的图像：![logistic代价函数图像2](..\logistic代价函数图像2.JPG)

可以简写单项代价函数为一个函数：
$$
Cost(h_\theta(x),y) = -ylog(h_\theta(x)) - (1-y)log(1-h_\theta(x))
$$
Logistic Function 的梯度下降更新参数
$$
\theta_j := \theta_j - \alpha\sum^m_{i = 1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j
$$

梯度下降向量化表示：
$$
\frac{\partial}{\partial\theta}J(\theta) = \frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j
$$


#### 多元分类

即去除其中一类数据，把其他数据看作一类，分别建立预测函数，当给出一个需要预测的数据时，对每个预测函数进行求值然后选择最大的那一个

### 正则化

#### 过拟合

过拟合曲线会经过所有训练数据点，但是对于新的数据却不一定拟合的好。

#### 新的代价函数

为了避免过拟合现象的情况发生，我们更新一下代价函数，在代价函数的后面加一个“正则项”，目的是尽量减少参数θ的大小，这样可以使得拟合函数更简洁并且过拟合的可能性更小。

线性回归添加了正则化后的代价函数：
$$
J(\theta_0, \theta_1) = \frac{1}{2m}[\sum_{i = 1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})}^2 + \lambda\sum^n_{j = 1}\theta_j^2]
$$
添加的一项叫做正则化项，λ是正则化参数，为了使所有参数（θ）变小，它的作用是平衡两个目标：

1. 假设函数对数据集的拟合
2. 假设函数中的参数尽量小

#### 线性回归的正则化

#### ![正则化线性回归梯度下降更新](..\正则化线性回归梯度下降更新.JPG)

#### Logistic函数的正则化

![正则化Logistic函数梯度下降更新](..\正则化Logistic函数梯度下降更新.JPG)

### 神经网络

#### 神经网络模型展示

![神经网络原理图解](..\神经网络原理图解.JPG)
$$
a^{(j)}_i表示第j层的第i个激活项
$$
$$
\theta^{(j)}是权重矩阵，控制着某一层到下一层的映射关系
$$

矩阵θ的维度取决于前后两层神经元的维度，前一层维度为s，后一层维度为s'，那么矩阵θ的维度为s' × （s + 1） （加上1是因为s还有一个x0没画在上图中）![前向传播](..\前向传播.JPG)

### 神经网络代价函数

#### 有多个输出的神经网络代价函数

![神经网络代价函数](..\神经网络代价函数.JPG)

注意后面的正则项是从1开始算的

#### 数据集中只有一组数据的反向传播算法

![反向传播算法](..\反向传播算法.JPG)
$$
对于g'(z^{(3)})来说，它等于a^{(3)}.*(1-a^{(3)})
$$

#### 数据集中有m组数据的反向传播算法![m组数据反向传播](..\m组数据反向传播.JPG)

红色部分是向量写法

j = 0时对应的是偏置项，所以没有加正则项